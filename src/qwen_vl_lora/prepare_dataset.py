"""
Data Preparation for Qwen-VL LoRA Fine-tuning
Convert VideoBadminton_Dataset to Qwen-VL training format
"""

import json
import os
import random
from pathlib import Path
import cv2
from tqdm import tqdm
import shutil

class QwenVLDatasetPreparator:
    """Prepare VideoBadminton_Dataset for Qwen-VL LoRA fine-tuning"""
    
    def __init__(self, 
                 source_dataset_path="/home/ubuntu/shuttle-sense/VideoBadminton_Dataset",
                 output_path="data/qwen_vl_dataset"):
        self.source_path = Path(source_dataset_path)
        self.output_path = Path(output_path)
        self.output_path.mkdir(exist_ok=True, parents=True)
        
        # Action class mapping (simplified names for better recognition)
        self.action_mapping = {
            "00_Short Serve": "çŸ­å‘çƒ",
            "01_Cross Court Flight": "æ–œçº¿é£è¡Œ",
            "02_Lift": "æŒ‘çƒ",
            "03_Tap Smash": "ç‚¹æ€",
            "04_Block": "æŒ¡çƒ",
            "05_Drop Shot": "åŠçƒ",
            "06_Push Shot": "æ¨çƒ",
            "07_Transitional Slice": "è¿‡æ¸¡åˆ‡çƒ",
            "08_Cut": "åˆ‡çƒ",
            "09_Rush Shot": "æ‰‘çƒ",
            "10_Defensive Clear": "é˜²å®ˆé«˜è¿œçƒ",
            "11_Defensive Drive": "é˜²å®ˆå¹³æŠ½",
            "12_Clear": "é«˜è¿œçƒ",
            "13_Long Serve": "é•¿å‘çƒ",
            "14_Smash": "æ€çƒ",
            "15_Flat Shot": "å¹³å°„çƒ",
            "16_Rear Court Flat Drive": "ååœºå¹³æŠ½",
            "17_Short Flat Shot": "çŸ­å¹³å°„"
        }
        
        # Various question templates for data augmentation
        self.question_templates = [
            "è¯·è¯†åˆ«è§†é¢‘ä¸­çš„ç¾½æ¯›çƒåŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Ÿ",
            "è§†é¢‘ä¸­å±•ç¤ºçš„æ˜¯ä»€ä¹ˆç¾½æ¯›çƒæŠ€æœ¯åŠ¨ä½œï¼Ÿ",
            "è¿™ä¸ªç¾½æ¯›çƒè§†é¢‘ç‰‡æ®µæ˜¾ç¤ºçš„åŠ¨ä½œç±»å‹æ˜¯ï¼Ÿ",
            "è¯·åˆ†æè¿™ä¸ªç¾½æ¯›çƒåŠ¨ä½œå¹¶ç»™å‡ºåç§°ã€‚",
            "è¯·é—®è§†é¢‘ä¸­çš„çƒå‘˜åšçš„æ˜¯ä»€ä¹ˆæŠ€æœ¯åŠ¨ä½œï¼Ÿ",
            "è¯†åˆ«ä¸€ä¸‹è¿™ä¸ªç¾½æ¯›çƒåŠ¨ä½œçš„ç±»å‹ã€‚",
            "è¿™æ˜¯ä»€ä¹ˆç¾½æ¯›çƒæŠ€æœ¯ï¼Ÿ",
            "è¯·æè¿°è§†é¢‘ä¸­çš„ç¾½æ¯›çƒåŠ¨ä½œåç§°ã€‚"
        ]
    
    def validate_video(self, video_path):
        """Validate if video file is readable"""
        try:
            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                return False
            
            # Try to read first frame
            ret, frame = cap.read()
            cap.release()
            return ret and frame is not None
        except:
            return False
    
    def copy_and_validate_videos(self):
        """Copy videos to output directory and validate them"""
        videos_dir = self.output_path / "videos"
        videos_dir.mkdir(exist_ok=True)
        
        valid_videos = []
        invalid_videos = []
        
        print("ğŸ“¹ Processing and validating videos...")
        
        for action_class in tqdm(self.action_mapping.keys(), desc="Processing classes"):
            class_dir = self.source_path / action_class
            if not class_dir.exists():
                continue
            
            video_files = list(class_dir.glob("*.mp4"))
            
            for video_file in tqdm(video_files, desc=f"Processing {action_class}", leave=False):
                # Create new filename with class prefix for uniqueness
                new_filename = f"{action_class}_{video_file.name}"
                dest_path = videos_dir / new_filename
                
                # Copy video
                if not dest_path.exists():
                    shutil.copy2(video_file, dest_path)
                
                # Validate video
                if self.validate_video(dest_path):
                    valid_videos.append({
                        'path': f"videos/{new_filename}",
                        'action_class': action_class,
                        'action_name': self.action_mapping[action_class]
                    })
                else:
                    invalid_videos.append(str(dest_path))
                    if dest_path.exists():
                        dest_path.unlink()  # Remove invalid video
        
        print(f"âœ… Valid videos: {len(valid_videos)}")
        print(f"âŒ Invalid videos: {len(invalid_videos)}")
        
        return valid_videos
    
    def create_training_data(self, valid_videos, train_ratio=0.8):
        """Create training data in Qwen-VL format"""
        
        # Split data
        random.shuffle(valid_videos)
        split_idx = int(len(valid_videos) * train_ratio)
        
        train_videos = valid_videos[:split_idx]
        val_videos = valid_videos[split_idx:]
        
        print(f"ğŸ“Š Dataset split:")
        print(f"  Training: {len(train_videos)} videos")
        print(f"  Validation: {len(val_videos)} videos")
        
        # Create training data
        train_data = self._create_conversation_data(train_videos, "training")
        val_data = self._create_conversation_data(val_videos, "validation")
        
        # Save training data
        train_file = self.output_path / "train.json"
        val_file = self.output_path / "val.json"
        
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, ensure_ascii=False, indent=2)
        
        with open(val_file, 'w', encoding='utf-8') as f:
            json.dump(val_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ Training data saved to: {train_file}")
        print(f"ğŸ’¾ Validation data saved to: {val_file}")
        
        return train_data, val_data
    
    def _create_conversation_data(self, videos, split_type):
        """Create conversation format data for Qwen-VL"""
        conversations = []
        
        for video in tqdm(videos, desc=f"Creating {split_type} data"):
            # Create multiple examples per video with different questions
            num_questions = random.randint(1, 3) if split_type == "training" else 1
            
            for _ in range(num_questions):
                question = random.choice(self.question_templates)
                answer = video['action_name']
                
                # Create different answer variations for training
                if split_type == "training":
                    answer_variations = [
                        answer,
                        f"è¿™æ˜¯{answer}",
                        f"è§†é¢‘ä¸­çš„åŠ¨ä½œæ˜¯{answer}",
                        f"è¿™ä¸ªåŠ¨ä½œå«åš{answer}",
                        f"çƒå‘˜æ‰§è¡Œçš„æ˜¯{answer}åŠ¨ä½œ"
                    ]
                    answer = random.choice(answer_variations)
                
                conversation = {
                    "id": f"{video['action_class']}_{Path(video['path']).stem}_{random.randint(1000, 9999)}",
                    "video": video['path'],
                    "conversations": [
                        {
                            "from": "user",
                            "value": f"<video>{question}"
                        },
                        {
                            "from": "assistant", 
                            "value": answer
                        }
                    ]
                }
                conversations.append(conversation)
        
        return conversations
    
    def create_class_statistics(self, valid_videos):
        """Create statistics about the dataset"""
        stats = {}
        for video in valid_videos:
            action_class = video['action_class']
            action_name = video['action_name']
            
            if action_name not in stats:
                stats[action_name] = {
                    'class_id': action_class,
                    'count': 0,
                    'videos': []
                }
            
            stats[action_name]['count'] += 1
            stats[action_name]['videos'].append(video['path'])
        
        # Save statistics
        stats_file = self.output_path / "dataset_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ˆ Dataset Statistics:")
        for action_name, stat in sorted(stats.items(), key=lambda x: x[1]['count'], reverse=True):
            print(f"  {action_name}: {stat['count']} videos")
        
        return stats
    
    def prepare_dataset(self):
        """Main function to prepare the dataset"""
        print("ğŸ¸ Preparing VideoBadminton_Dataset for Qwen-VL LoRA fine-tuning")
        print("=" * 70)
        
        # Step 1: Copy and validate videos
        valid_videos = self.copy_and_validate_videos()
        
        if not valid_videos:
            print("âŒ No valid videos found!")
            return False
        
        # Step 2: Create training data
        train_data, val_data = self.create_training_data(valid_videos)
        
        # Step 3: Create statistics
        stats = self.create_class_statistics(valid_videos)
        
        # Step 4: Create README
        self._create_readme(len(train_data), len(val_data), len(stats))
        
        print("\nâœ… Dataset preparation completed!")
        print(f"ğŸ“ Output directory: {self.output_path}")
        print(f"ğŸ“Š Training samples: {len(train_data)}")
        print(f"ğŸ“Š Validation samples: {len(val_data)}")
        
        return True
    
    def _create_readme(self, train_count, val_count, class_count):
        """Create README file for the dataset"""
        readme_content = f"""# Qwen-VL ç¾½æ¯›çƒåŠ¨ä½œè¯†åˆ«æ•°æ®é›†

## æ•°æ®é›†æ¦‚è¿°

æœ¬æ•°æ®é›†åŸºäº VideoBadminton_Dataset åˆ›å»ºï¼Œä¸“é—¨ç”¨äº Qwen-VL æ¨¡å‹çš„ LoRA å¾®è°ƒï¼Œå®ç°ç¾½æ¯›çƒåŠ¨ä½œè¯†åˆ«åŠŸèƒ½ã€‚

## æ•°æ®ç»Ÿè®¡

- **è®­ç»ƒæ ·æœ¬**: {train_count} ä¸ª
- **éªŒè¯æ ·æœ¬**: {val_count} ä¸ª
- **åŠ¨ä½œç±»åˆ«**: {class_count} ç§ç¾½æ¯›çƒæŠ€æœ¯åŠ¨ä½œ

## æ•°æ®æ ¼å¼

æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ï¼š
- è§†é¢‘è·¯å¾„
- é—®ç­”å¯¹è¯ï¼ˆç”¨æˆ·é—®é¢˜ + æ¨¡å‹å›ç­”ï¼‰

ç¤ºä¾‹æ ¼å¼ï¼š
```json
{{
  "id": "unique_id",
  "video": "videos/video_file.mp4",
  "conversations": [
    {{
      "from": "user",
      "value": "<video>è¯·è¯†åˆ«è§†é¢‘ä¸­çš„ç¾½æ¯›çƒåŠ¨ä½œæ˜¯ä»€ä¹ˆï¼Ÿ"
    }},
    {{
      "from": "assistant",
      "value": "æ€çƒ"
    }}
  ]
}}
```

## æ–‡ä»¶ç»“æ„

```
{self.output_path}/
â”œâ”€â”€ videos/              # è§†é¢‘æ–‡ä»¶ç›®å½•
â”œâ”€â”€ train.json          # è®­ç»ƒæ•°æ®
â”œâ”€â”€ val.json            # éªŒè¯æ•°æ®
â”œâ”€â”€ dataset_stats.json  # æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
â””â”€â”€ README.md           # æœ¬æ–‡ä»¶
```

## åŠ¨ä½œç±»åˆ«

åŒ…å« 18 ç§ç¾½æ¯›çƒæŠ€æœ¯åŠ¨ä½œï¼š
{chr(10).join([f'- {chinese}' for chinese in self.action_mapping.values()])}

## ä½¿ç”¨è¯´æ˜

1. ç¡®ä¿æ‰€æœ‰è§†é¢‘æ–‡ä»¶åœ¨ `videos/` ç›®å½•ä¸‹
2. ä½¿ç”¨ `train.json` è¿›è¡Œ LoRA å¾®è°ƒè®­ç»ƒ
3. ä½¿ç”¨ `val.json` è¿›è¡ŒéªŒè¯å’Œæµ‹è¯•

## æ•°æ®è´¨é‡

- æ‰€æœ‰è§†é¢‘æ–‡ä»¶éƒ½å·²éªŒè¯å¯è¯»æ€§
- é—®é¢˜æ¨¡æ¿å¤šæ ·åŒ–ï¼Œå¢å¼ºæ¨¡å‹æ³›åŒ–èƒ½åŠ›
- ç­”æ¡ˆæ ¼å¼æ ‡å‡†åŒ–ï¼Œä¾¿äºæ¨¡å‹å­¦ä¹ 
"""
        
        readme_file = self.output_path / "README.md"
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme_content)

def main():
    """Main function"""
    preparator = QwenVLDatasetPreparator()
    success = preparator.prepare_dataset()
    
    if success:
        print("\nğŸ¯ ä¸‹ä¸€æ­¥ï¼š")
        print("1. æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®ï¼šdata/qwen_vl_dataset/")
        print("2. å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ")
        print("3. è¿è¡Œæ¨ç†æµ‹è¯•")
    else:
        print("âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥æºæ•°æ®è·¯å¾„")

if __name__ == "__main__":
    main()